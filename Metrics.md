# Evaluation through Common Literature Metrics
In order to provide a complete evaluation through well-known metrics (and also adaptations), responses from the model were also evaluated using:
- Rouge-L [[1]](#1)
- Ragas' Semantic Similarity (w/ [sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2))
- BERTScore (Recall, Precision and F1) [[2]](#2)
- Perplexity (PPL)

As the metrics above cannot directly show if the answers given are correct, an adaptation to the metric was proposed.
To do that, three wrong answers were created for each entry in the dataset.
Each wrong answer presents a variation of the correct answer, however, with incorrect information.
Examples of entries in the dataset can be seen in the table below.

TABLE

In order to evaluate the answers, the base metric is used to provide scores for each of the answers (correct and incorrect) of a question in the dataset
Then, the answer with the best score (for BERTScore, the highest score; for Perplexity, the lowest score) is selected as the option the assistant would deem most correct out of them.
The new types of metrics are:

- **BERTScore Multiple Choice Recall/Precision/F1**: the system-generated answer and the wrong answers are compared to the correct answer using BERTScore. The highest scoring answer is the one closest to the correct one. If this answer is not the one generated by the system, the model does not pontuate.
- **Multiple Choice Perplexity**: the PPL value is calculated for the correct and incorrect answers in the dataset. The answer with the lowest PPL value is deemed the one the system would most likely answer. If the answer is not the correct one from the dataset, the model does not pontuate.

## Experiment 1: Choosing the best model combination for the assistant
![scores_exp1](https://github.com/user-attachments/assets/760c9fb6-3ce6-4288-a9f0-117b1267d31e)

## Experiment 1.1: Choosing the best parametrization for the context initialization
![scores_exp1_1](https://github.com/user-attachments/assets/d32e0dd9-9e3a-4270-8b25-4187affc0fed)

## Experiment 1.2: Choosing the best parametrization for the search
![scores_exp1_2](https://github.com/user-attachments/assets/f7b798c5-2a7e-4165-a627-e475c2e6d223)

## Experiment 2: Analyzing the efficiency of the summary extension
![scores_exp2](https://github.com/user-attachments/assets/d0f1e3eb-c69b-46df-8725-dc56173243cf)

## Experiment 3: Final Experiment
![scores_exp3](https://github.com/user-attachments/assets/f5fddef8-05c3-48fa-b37c-ad3d437d2f9e)

# References
<a id="1">[1]</a>
Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 605â€“612, Barcelona, Spain.

<a id="2">[2]</a>
Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. 1904.09675. https://arxiv.org/abs/1904.09675
